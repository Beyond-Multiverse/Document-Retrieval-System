{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7737ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\My\n",
      "[nltk_data]     Computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\My\n",
      "[nltk_data]     Computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing all the necessary libraries\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from heapq import nlargest\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d65837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_open(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        loader = pickle.load(f)\n",
    "    f.close()\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90b89b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=pickle_open(\"term_frequency.pkl\")\n",
    "df=pickle_open(\"doc_freq.pkl\")\n",
    "file_index=pickle_open(\"file_index.pkl\")\n",
    "doc_len=pickle_open(\"doc_len.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac9324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_query = {}\n",
    "query = open(sys.argv[1],'r')\n",
    "for q in query:\n",
    "    content = q.split(\"\\t\")\n",
    "    all_query[content[1].replace('\\n','')] = content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34cb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = {key: [] for key in all_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "997db5de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b=0.75\n",
    "k=1.2\n",
    "total_files=len(file_index)\n",
    "\n",
    "for q in all_query:\n",
    "    query = q.lower()\n",
    "    query = re.sub('\\W+|_', ' ', query)\n",
    "    ps = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stopword_list = [stopword.lower() for stopword in stop_words]\n",
    "    tokens = word_tokenize(query)\n",
    "    stems = [ps.stem(token) for token in tokens]\n",
    "    words = [word for word in stems if word not in stopword_list]\n",
    "\n",
    "    score = dict()\n",
    "    doc_len_mean = np.array(list(doc_len.values())).mean()\n",
    "    for file_id in range(len(file_index)):\n",
    "        score[file_id]=0\n",
    "        for word in words:\n",
    "            TF = 0\n",
    "            if word in tf :\n",
    "                if file_id in tf[word] :\n",
    "                    TF = tf[word][file_id] \n",
    "            NDF = df[word] if word in df else 0\n",
    "            IDF = np.log((total_files-NDF+0.5)/(NDF+0.5))\n",
    "            numerator = TF*IDF*(k+1)\n",
    "            denominator = TF+k*(1-b+b*(doc_len[file_id]/doc_len_mean))\n",
    "            score[file_id]+=(numerator/denominator)\n",
    "    \n",
    "    res = nlargest(10, score, key = score.get)\n",
    "    res = [file_index[fid] for fid in res]\n",
    "    retrieved_docs[q]=res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a8b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid=[]\n",
    "iteration=[]\n",
    "docid=[]\n",
    "relevance=[]\n",
    "\n",
    "for q in retrieved_docs:\n",
    "    for doc in retrieved_docs[q]:\n",
    "        qid.append(all_query[q])\n",
    "        iteration.append(1)\n",
    "        docid.append(doc)\n",
    "        relevance.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4bb7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'QueryId' : qid,\n",
    "    'Iteration' : iteration,\n",
    "    'DocId' : docid,\n",
    "    'Relevance' : relevance\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Q4/QRels-BM25.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
