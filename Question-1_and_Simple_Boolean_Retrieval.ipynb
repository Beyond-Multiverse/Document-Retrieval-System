{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a11c5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\My\n",
      "[nltk_data]     Computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\My\n",
      "[nltk_data]     Computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing all the necessary libraries\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from heapq import nlargest\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734c157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfFiles = PlaintextCorpusReader('english-corpora', '.*')\n",
    "total_files = len(listOfFiles.fileids())\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stopword_list = [stopword.lower() for stopword in stop_words]\n",
    "doc_norm={}\n",
    "combined_doc=[]\n",
    "unique_words = set()\n",
    "file_index = {}\n",
    "doc_len = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215afe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    content = re.sub(r'([.][a-zA-Z]{1,}[-])|([.][a-z]{1,}.+[{].+[}])|([<][a-z ]{1,}.+[/][>])','',content)\n",
    "    content = content.lower()\n",
    "    content = content.replace('\\\\n', ' ')\n",
    "    content = content.replace('\\n', ' ')\n",
    "    content = content.replace('\\t', ' ')\n",
    "    content = re.sub('\\W+|_', ' ', content)\n",
    "    tokens = word_tokenize(content)\n",
    "    tokens = [token for token in tokens if len(tokens)>1]\n",
    "    stems = [ps.stem(token) for token in tokens]\n",
    "    syllables = [syllable for syllable in stems if syllable not in stopword_list]\n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a20fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = -1\n",
    "\n",
    "#Iterating for all files\n",
    "for file in listOfFiles.fileids():\n",
    "    \n",
    "    file_id += 1\n",
    "    \n",
    "    #fetching the source file location and name\n",
    "    plain_doc=\"english-corpora/\"+str(file)\n",
    "    file_index[file_id] = plain_doc\n",
    "    with open(plain_doc, 'r', encoding='UTF-8') as f:\n",
    "\n",
    "        syllables = preprocess(f.read())\n",
    "\n",
    "    doc_len[file_id] = len(syllables)\n",
    "    combined_doc.append(syllables)\n",
    "    unique_words = unique_words.union(set(syllables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a549195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = -1\n",
    "tf = {word: {} for word in unique_words}\n",
    "df = dict.fromkeys(unique_words, 0)\n",
    "presence = dict.fromkeys(unique_words, set())\n",
    "#Iterating for all files\n",
    "for file in listOfFiles.fileids():\n",
    "    \n",
    "    file_id += 1\n",
    "    \n",
    "    individual_doc_freq = Counter(combined_doc[file_id])\n",
    "    \n",
    "    for word in individual_doc_freq.keys():\n",
    "        \n",
    "        presence[word] = presence[word].union({file})\n",
    "        tf[word][file_id]=individual_doc_freq[word]\n",
    "        df[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1044c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = -1\n",
    "\n",
    "for doc in combined_doc:\n",
    "    file_id+=1\n",
    "    val=0\n",
    "    for word in set(doc):\n",
    "        val+=np.square(doc.count(word)*np.log(total_files/df[word]))\n",
    "    doc_norm[file_id]=(np.sqrt(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5a8a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_save(file_name, dumper):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(dumper, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3223d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_save('file_index.pkl', file_index)\n",
    "pickle_save('doc_len.pkl', doc_len)\n",
    "pickle_save('term_frequency.pkl', tf)\n",
    "pickle_save('doc_freq.pkl', df)\n",
    "pickle_save('doc_norm.pkl', doc_norm)\n",
    "pickle_save('combined_doc.pkl', combined_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efc0b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_query = {}\n",
    "query = open(sys.argv[1],'r')\n",
    "for q in query:\n",
    "    content = q.split(\"\\t\")\n",
    "    all_query[content[1].replace('\\n','')] = content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3488be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = {key: [] for key in all_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5943621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files=len(file_index)\n",
    "for q in all_query:\n",
    "    query = q.lower()\n",
    "    query = re.sub('\\W+|_', ' ', query)\n",
    "    ps = PorterStemmer()\n",
    "    stopword_list = set(stopwords.words('english'))\n",
    "    stopword_list = [stopword.lower() for stopword in stopword_list]\n",
    "    tokens = word_tokenize(query)\n",
    "    stems = [ps.stem(token) for token in tokens]\n",
    "    words = [word for word in stems if word not in stopword_list]\n",
    "\n",
    "    ans = set()\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i] == \"or\" :\n",
    "            continue\n",
    "        if words[i] == \"and\" :\n",
    "            i+=1\n",
    "            if words[i] in presence.keys() and i!=len(words):\n",
    "                ans = ans.intersection(presence[words[i]])\n",
    "            continue\n",
    "        if words[i] == \"not\" :\n",
    "            i+=1\n",
    "            if words[i] in presence.keys() and i!=len(words):\n",
    "                ans = ans.difference(presence[words[i]])\n",
    "            continue\n",
    "        if words[i] in presence.keys():\n",
    "            ans = ans.union(presence[words[i]])\n",
    "        \n",
    "        \n",
    "        res = ['english-corpora/' + sub for sub in ans]\n",
    "        retrieved_docs[q]=res[:20]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbf7171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid=[]\n",
    "iteration=[]\n",
    "docid=[]\n",
    "relevance=[]\n",
    "\n",
    "for q in retrieved_docs:\n",
    "    for doc in retrieved_docs[q]:\n",
    "        qid.append(all_query[q])\n",
    "        iteration.append(1)\n",
    "        docid.append(doc)\n",
    "        relevance.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90839422",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'QueryId' : qid,\n",
    "    'Iteration' : iteration,\n",
    "    'DocId' : docid,\n",
    "    'Relevance' : relevance\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Q4/QRels-Simple_Boolean_Retrieval.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de784e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
