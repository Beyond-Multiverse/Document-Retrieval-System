{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20587a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\My\n",
      "[nltk_data]     Computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\My\n",
      "[nltk_data]     Computer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing all the necessary libraries\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from heapq import nlargest\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db307e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pickle_open(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        loader = pickle.load(f)\n",
    "    f.close()\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531bcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pickle_open(\"doc_freq.pkl\")\n",
    "file_index=pickle_open(\"file_index.pkl\")\n",
    "doc_norm=pickle_open(\"doc_norm.pkl\")\n",
    "combined_doc=pickle_open(\"combined_doc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989e7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_query = {}\n",
    "query = open(sys.argv[1],'r')\n",
    "for q in query:\n",
    "    content = q.split(\"\\t\")\n",
    "    all_query[content[1].replace('\\n','')] = content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4369579",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = {key: [] for key in all_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f74649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files=len(file_index)\n",
    "for q in all_query:\n",
    "    query = q.lower()\n",
    "    query = re.sub('\\W+|_', ' ', query)\n",
    "    ps = PorterStemmer()\n",
    "    stopword_list = set(stopwords.words('english'))\n",
    "    stopword_list = [stopword.lower() for stopword in stopword_list]\n",
    "    tokens = word_tokenize(query)\n",
    "    stems = [ps.stem(token) for token in tokens]\n",
    "    words = [word for word in stems if word not in stopword_list]\n",
    "\n",
    "\n",
    "    TF_IDF = []\n",
    "    for word in words:\n",
    "        TF_IDF = np.append(TF_IDF,(words.count(word)*np.log(total_files/df[word])))\n",
    "\n",
    "    TF_IDF=np.array(TF_IDF)/np.linalg.norm(TF_IDF)\n",
    "\n",
    "    score = dict()\n",
    "\n",
    "    for file in range(total_files):\n",
    "        doc_vector=[]\n",
    "        for word in words:\n",
    "            tf_idf=(combined_doc[file].count(word)*np.log(total_files/df[word]))\n",
    "            doc_vector.append(tf_idf)\n",
    "        doc_vector=np.array(doc_vector)/doc_norm[file]\n",
    "        score[file]=np.dot(TF_IDF,doc_vector)\n",
    "\n",
    "    res = nlargest(10, score, key = score.get)\n",
    "    res = [file_index[fid] for fid in res]\n",
    "    retrieved_docs[q]=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e325fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid=[]\n",
    "iteration=[]\n",
    "docid=[]\n",
    "relevance=[]\n",
    "\n",
    "for q in retrieved_docs:\n",
    "    for doc in retrieved_docs[q]:\n",
    "        qid.append(all_query[q])\n",
    "        iteration.append(1)\n",
    "        docid.append(doc)\n",
    "        relevance.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855f127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'QueryId' : qid,\n",
    "    'Iteration' : iteration,\n",
    "    'DocId' : docid,\n",
    "    'Relevance' : relevance\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Q4/QRels-TFIDF.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
